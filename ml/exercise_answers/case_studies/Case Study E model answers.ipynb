{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1><font size=7> Case Study E</font> </h1> </center>\n",
    "\n",
    "# Clustering Facebook Posts - Example Answer\n",
    "\n",
    "This notebook contains a minimum example response to the questions posed in Case Study D. These are not the only ways to approach the problems, but should hopefully point you in a fruitful direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load and explore the variables distributions in the data set, convert `\"status_published\"` to a datetime if it is not already.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(\"../../data/fb_sale_posts.csv\")\n",
    "\n",
    "posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring: Status Published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"status_published\"] = pd.to_datetime(posts[\"status_published\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"status_published\"].value_counts().sort_index().cumsum().plot()\n",
    "plt.title(\"Cumulative sum of posts\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"status_published\"].hist(bins=50)\n",
    "plt.title(\"Indicator of post frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring: status id\n",
    "\n",
    "Unique ID, so distribution isn't interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(posts))\n",
    "posts[\"status_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring: status type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"status_type\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Count of status types\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring: num reactions, num likes, num loves, num wows, num hahas, num sads and num angrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"num_reactions\"].hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly skewed distribution. Potentially a powerlaw or exponential decay function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"num_likes\", \"num_loves\", \"num_wows\", \"num_hahas\", \"num_sads\", \"num_angrys\"]\n",
    "\n",
    "print(\"Cutting is end exclusive - '0 to 1' is 0, '1 to 10' is 1-9\")\n",
    "\n",
    "for column in columns:\n",
    "    plt.figure()\n",
    "    # bin the data\n",
    "    out = pd.cut(posts[column], bins=[0, 1, 10, 100, 1000, 10000], include_lowest=True)\n",
    "    # count how many are in each bin\n",
    "    ax = out.value_counts(sort=False).plot.bar(rot=45, color=\"b\", figsize=(6,4))\n",
    "    # change the label to be easier to understand from interval notation\n",
    "    ax.set_xticklabels([(str(int(c.left)) +\" to \" + str(int(c.right))) for c in out.cat.categories])\n",
    "    # set limit to allow comparison\n",
    "    ax.set_ylim(0,len(posts))\n",
    "    plt.title(f\"{column} distribution\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore: num comments, num shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the data\n",
    "out = pd.cut(posts[\"num_comments\"], bins=[0, 1, 10, 100, 1000, 10000], include_lowest=True)\n",
    "# count how many are in each bin\n",
    "ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"g\", figsize=(6,4))\n",
    "# change the label to be easier to understand from interval notation\n",
    "ax.set_xticklabels([(str(int(c.left)) +\" to \" + str(int(c.right))) for c in out.cat.categories])\n",
    "plt.xticks(rotation=30)\n",
    "# set limit to allow comparison\n",
    "ax.set_ylim(0,len(posts))\n",
    "plt.title(\"num_comments distribution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More posts with high numbers of comments than reactions. Fewer posts with no comments compared to reaction breakdowns. Some posts do extremely well but the majority have very little interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bin the data\n",
    "out = pd.cut(posts[\"num_shares\"], bins=[0, 1, 10, 100, 1000, 10000], include_lowest=True)\n",
    "# count how many are in each bin\n",
    "ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"g\", figsize=(6,4))\n",
    "# change the label to be easier to understand from interval notation\n",
    "ax.set_xticklabels([(str(int(c.left)) +\" to \" + str(int(c.right))) for c in out.cat.categories])\n",
    "plt.xticks(rotation=30)\n",
    "# set limit to allow comparison\n",
    "ax.set_ylim(0,len(posts))\n",
    "plt.title(\"num_comments distribution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar trend to the comments, but not as extreme a tail. Interestingly more posts with 100 to 1000 shares than 10 to 100 and comparable to 1 to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Some of the variables are highly skewed numerical distributions. Find methods in `sklearn.preprocessing` or elsewhere to change the distribution of data from skewed to uniform or normal. Use the methods on `\"num_comments\"`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a range of methods in `sklearn` and elsewhere than can help us in these sorts of situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to normal distribution:\n",
    "\n",
    "We can use the PowerTransformer with the `\"box-cox\"` method. For more information on this transformation see [this tutorial](https://www.itl.nist.gov/div898/handbook/eda/section3/eda336.htm). However, the box-cox transformation is defined only for strictly positive values (excludes negative values and zero), but our data contains many zero values. For this reason we will use the `\"yeo-johnson\"` method. The transformation itself is given in [the sklearn documentation](https://bookdown.org/max/FES/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method=\"yeo-johnson\", standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"num_comments_yeo\"] = pt.fit_transform(posts[[\"num_comments\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"num_comments_yeo\"].hist()\n",
    "plt.title(\"Yeo-Johnson transformation on 'num_comments'\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is *more* normally distributed than the orginal data, but as we can see this has not created a perfectly normal distribution. Lets compare it to the original data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"num_comments\"].hist()\n",
    "plt.title(\"Non-transformed 'num_comments'\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common approach to this problem would to be to perform a log transformation to reduce the range of the data.\n",
    "\n",
    "A log function is not defined non-positive values, so we often use `np.log1p` - the `1p` means \"one plus\". We add 1 to make the data all positive.\n",
    "\n",
    "Again, we can see that this does not produce a perfect normal distribution in the case of our data (it may be too skewed) - but the spread of data is now much less skewed, and will be easier for models to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using numpy log1p function to the whole column\n",
    "posts[\"num_comments\"].apply(np.log1p).hist()\n",
    "plt.title(\"log1p transformation of 'num_comments'\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to uniform distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a quantile transformation, which splits our data into... quantiles to map to different distributions. In this case we will aim to make a uniform transformation from `\"num_comments\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# use default parameters\n",
    "qt = QuantileTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"num_comments_uniform\"] = qt.fit_transform(posts[[\"num_comments\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore distribution post-transformation\n",
    "posts[\"num_comments_uniform\"].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data is bounded by `[0,1]`. Due to the extreme number of zeros we haven't split the lowest quantile. The rest of the values are distributed uniformly across the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. The \"num_reactions\" should be a sum of all the other reactions recorded per record. Create new features that are the proportion of each reaction type, within the values of 0-1. For example  $𝑝𝑟𝑜𝑝\\_𝑎𝑛𝑔𝑟𝑦𝑠 = \\frac{𝑛𝑢𝑚\\_𝑎𝑛𝑔𝑟𝑦𝑠}{𝑛𝑢𝑚\\_𝑟𝑒𝑎𝑐𝑡𝑖𝑜𝑛𝑠}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column for each reaction type\n",
    "for column in columns:\n",
    "    # create new name for column using prop_ \n",
    "    new_col_name = \"prop_\" + column.split(\"_\")[1]\n",
    "    # The proportion is the count of specific breakdown / all reactions\n",
    "    posts[new_col_name] = posts[column] / posts[\"num_reactions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = posts[posts.columns[posts.columns.str.startswith(\"prop_\")]]\n",
    "proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean proportion of each reaction type\n",
    "proportions.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Explore covariances between variables: `\"num_reactions\", \"num_comments\", \"num_shares\", \"num_likes\", \"num_loves\"`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_exploration = posts[[\"num_reactions\", \"num_comments\", \"num_shares\", \"num_likes\", \"num_loves\"]]\n",
    "covariance_exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_exploration.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_exploration.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a few of the insights we can gain from the above:\n",
    "\n",
    "* num_reactions and num_likes are highly colinear. This is because num_reactions is the sum of all reaction types, and likes are by far the largest reaction. We should keep this colinearity in mind when modelling.\n",
    "* The number of loves is correlated with the comments and shares, more so than the number of likes. This implies that the reactions other than like may be used in different situations than likes, rather than as a subset of likes.\n",
    "* Comments and shares are linearly correlated above 0.60 - which may indicate that they are well linked as types of interations. They are far more linked than with reactions.\n",
    "\n",
    "**Note** We have only looked at the featues without transformation here, further or different insights would be gathered from using the featues in a different transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>5. Create a new set of features and add them to the `posts` dataframe: \n",
    " * \"TimeSinceFirst\" - (int) count of days since the first day that appears in the data set\n",
    " * \"TimeOfDay\" - (int) hour the post was created\n",
    " * \"DayOfWeek\" - (str) the day of the week the post was made (Monday, Tuesday, Wednesday...)</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"TimeSinceFirst\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the earliest date in the data set\n",
    "first_date = posts[\"status_published\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"TimeSinceFirst\"] = (posts[\"status_published\"] - first_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"TimeSinceFirst\"].hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"TimeOfDay\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each timestamp has an associated hour\n",
    "posts[\"TimeOfDay\"] = posts[\"status_published\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of hours of the day\n",
    "posts[\"TimeOfDay\"].hist(bins=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question this raises is that does the number of posts peak at around 2am and 7am and is at a minimum at 4pm - or is there a time difference between where the data was recorded and Thailand? It will make a difference to our interpretation of the resulting models / analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"DayOfWeek\"**\n",
    "\n",
    "We could either convert to integers or to strings. Integers will enforce an interval between values, which makes sense in some sense. For example Monday to Tuesday is the same distance as Thursday to Friday. However, the distance from Sunday to Monday will be large, which is not true. This is due to the cyclical nature of week days. For this reason strings are taken instead and days of the week are treated as categorical for now. They could be binned to lower numbers of categories if necessary (weekend / not-weekend for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"DayOfWeek\"] = posts[\"status_published\"].dt.day_name()\n",
    "posts[\"DayOfWeek\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar numbers of posts across all days of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. How are the covariances/correlations with numerical features different between status_type == \"video\", \"photo\", particularly the numbers of reactions, comments and shares?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = posts[posts[\"status_type\"] == \"video\"]\n",
    "photo = posts[posts[\"status_type\"] == \"photo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video[[\"num_reactions\", \"num_comments\", \"num_shares\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo[[\"num_reactions\", \"num_comments\", \"num_shares\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.groupby(\"status_type\").size().reset_index().rename(columns={0: \"count\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations between different types of interaction are quite different between the different types of post interaction. \n",
    "Briefly:\n",
    "\n",
    "* The shares of videos are more correlated than with reactions than for photos\n",
    "* The shares and comments of videos are highly correlated compared to photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring the proportions of reaction counts we need to keep in mind some contraints of proportions.\n",
    "\n",
    "If we define a reaction as $react_i$, then the proportion of $react_i$ is $prop\\_react_i = \\frac{react_i}{num\\_reacts}$.\n",
    "\n",
    "As $\\sum^i{react_i} = num\\_reacts$\n",
    "\n",
    "Therefore $\\sum^i{prop\\_react_i} = 1$\n",
    "\n",
    "Each proportion has a maximum value of $1$, and the sum of any $react_i$'s must be less than or equal to 1. \n",
    "\n",
    "If we plot one proportion of reactions against another, out data will always appear on the line or below $react_y = -react_x + 1$\n",
    "\n",
    "As likes are by far the most dominant react, it may be useful to define the compliment proportion to likes, which is equivalent to the sum of non-likes proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"prop_not_likes\"] = 1 - posts[\"prop_likes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likes and non-likes have a relationship (as we defined it)\n",
    "plt.scatter(posts[\"prop_likes\"], posts[\"prop_not_likes\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As likes lower, the loves appear to increase\n",
    "plt.scatter(posts[\"prop_likes\"], posts[\"prop_loves\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There doesn't appear to be a clear relationship\n",
    "# The two reacts appear to some extent independent\n",
    "plt.scatter(posts[\"prop_angrys\"], posts[\"prop_loves\"])\n",
    "plt.xlim(0,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There appear to be some relationship between the proportions\n",
    "# of these two reacts, or at least a cluster of \n",
    "# values off the axis\n",
    "plt.scatter(posts[\"prop_wows\"], posts[\"prop_loves\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not a lot of crossovers of sad and happy reacts\n",
    "plt.scatter(posts[\"prop_hahas\"], posts[\"prop_sads\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When loves are non-zero there do appear to be haha's\n",
    "plt.scatter(posts[\"prop_loves\"], posts[\"prop_hahas\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Counts\n",
    "\n",
    "This is a different method than finding the proportions, but it follows a similar intuition. We will divide by the squared sum of each row, rather than the sum as previously (this was given by the total reactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting does nothing for normalizing (stateless, unlike other scalers)\n",
    "normal_reacts = Normalizer().fit_transform(posts[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example column vs column plot\n",
    "plt.scatter(normal_reacts[:,0], normal_reacts[:,1])\n",
    "plt.xlabel(columns[0])\n",
    "plt.ylabel(columns[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "# there are nicer plotting methods (such as in seaborn)\n",
    "# for plotting all columns against each other,\n",
    "# but this is how I would do it in python standard library\n",
    "combinations = list(combinations(enumerate(columns), 2))\n",
    "combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each combination and plot \n",
    "# the normalised counts\n",
    "for (idx1, col1), (idx2, col2) in combinations:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    # plot the index from the array corresponding the the column desired\n",
    "    plt.scatter(normal_reacts[:,idx1], normal_reacts[:,idx2])\n",
    "    plt.xlabel(col1)\n",
    "    plt.ylabel(col2)\n",
    "    plt.title(f\"{col1} and {col2}\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### Prep\n",
    "\n",
    "Lets make some functions that will help us with the repetative tasks of plotting and evaluating clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_data(X, labels=None, components=(0,1)):\n",
    "    \n",
    "    # using basic PCA\n",
    "    pca = PCA(random_state=1)\n",
    "        \n",
    "    X_red = pca.fit_transform(X)\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    # select components of reduced data, using cluster label as colouring\n",
    "    plt.scatter(X_red[:,components[0]], X_red[:,components[1]], c=labels)\n",
    "    \n",
    "    plt.xlabel(f\"Component {components[0]}\")\n",
    "    plt.ylabel(f\"Component {components[1]}\")\n",
    "    \n",
    "    # add the explained variance of each component selected\n",
    "    print(\"Explained variance ratio in shown components:\", \n",
    "          pca.explained_variance_ratio_[components[0]] + pca.explained_variance_ratio_[components[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def cluster_evaluate(X, cluster_object, tuning_param):\n",
    "    # initialize clustering object\n",
    "    clusterer = cluster_object(**tuning_param) # unpack whatever parameters fiven from a dictionary\n",
    "    \n",
    "    # generate predictions\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    # evaluate\n",
    "    score = silhouette_score(X=X, labels=cluster_labels)\n",
    "    \n",
    "    # return score and labels for exploration\n",
    "    return score, cluster_labels  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_values = list(range(2, 20))\n",
    "k_scores = []\n",
    "best_k = None\n",
    "best_score_kmeans = 0\n",
    "best_labels_kmeans = None\n",
    "\n",
    "for k in k_values:   \n",
    "    individual_score, found_labels = cluster_evaluate(normal_reacts, KMeans, {\"n_clusters\": k})\n",
    "    k_scores.append(individual_score)\n",
    "    \n",
    "    if individual_score > best_score_kmeans:\n",
    "        best_score_kmeans = individual_score\n",
    "        best_k = k\n",
    "        best_labels_kmeans = found_labels\n",
    "    \n",
    "print(f\"Best K: {best_k}, with score: {best_score_kmeans}\")\n",
    "    \n",
    "plt.plot(k_values, k_scores)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Silhouette score\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(normal_reacts, labels=best_labels_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our silhouette score is high, we have been unable to differentaite the three clear groupings that are clear from the first components. However, as we are only showing part of the data it may not actually be 3 clear clusters - although the displayed data is a significant proportion of the data's variance. Let's try a different method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "eps_values = np.linspace(0.05, 0.6, 10)\n",
    "eps_scores = []\n",
    "best_eps = None\n",
    "best_score_dbscan = 0\n",
    "best_labels_dbscan = None\n",
    "\n",
    "for eps in eps_values:   \n",
    "    individual_score, found_labels = cluster_evaluate(normal_reacts, DBSCAN, {\"eps\": eps})\n",
    "    eps_scores.append(individual_score)\n",
    "    \n",
    "    if individual_score > best_score_dbscan:\n",
    "        best_score_dbscan = individual_score\n",
    "        best_eps = eps\n",
    "        best_labels_dbscan = found_labels\n",
    "    \n",
    "print(f\"Best distance: {best_eps}, with score: {best_score_dbscan}\")\n",
    "    \n",
    "plt.plot(eps_values, eps_scores)\n",
    "plt.xlabel(\"Eps\")\n",
    "plt.ylabel(\"Silhouette score\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(normal_reacts, labels=best_labels_dbscan, components=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(best_labels_dbscan, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this investigation we can see that the DBSCAN algorithm out performs the KMeans clustering. In our first two components we can see that it better groups the majority cluster better, as it is not contrained in the same way as kmeans. At first glance there is a mixed cluster found, however, when exploring in differnet components you can see that the two differnetly label data near each other are actually separated in different components.\n",
    "\n",
    "The resulting clusters are two clusters with some (5) in my case labelled outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing to stick with ward linkage as default\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "k_values = list(range(2, 10))\n",
    "k_scores = []\n",
    "best_k = None\n",
    "best_score_agglom = 0\n",
    "best_labels_agglom = None\n",
    "\n",
    "for k in k_values:   \n",
    "    individual_score, found_labels = cluster_evaluate(normal_reacts, AgglomerativeClustering, {\"n_clusters\": k})\n",
    "    k_scores.append(individual_score)\n",
    "    \n",
    "    if individual_score > best_score_agglom:\n",
    "        best_score_agglom = individual_score\n",
    "        best_k = k\n",
    "        best_labels_agglom = found_labels\n",
    "    \n",
    "print(f\"Best K: {best_k}, with score: {best_score_agglom}\")\n",
    "    \n",
    "plt.plot(k_values, k_scores)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Silhouette score\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(normal_reacts, labels=best_labels_agglom, components=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us 2 clusters as optimal, rather than the 3 from kmeans, or 2 with outliers from DBSCAN. Looking at the first two components the two outlier clusters are grouped together with the half of the majority class. This could be a result of the linkage method, or an effect not shown in our components. Explore the result in components (0,3), there the clusters look like a much more reasonable choice. Further to this, the resulting clusters will be highly impacted by how we normalised the data. Data on a different scale / transformed in a different way will yield different results. Remember: we are not clustering using PCA, we are just using it to visualise the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what does this clustering tell us?\n",
    "\n",
    "My interpretation would be:\n",
    "\n",
    "* There is a clear majority cluster as shown in the visualisation, which can transfer to two classes in other dimensions.\n",
    "* There are some outlier clusters which show unusual / well separated values from the main cluster. They are however, not well separated in all dimensions and therefore are mixed in some cases\n",
    "* For the first principal component by far the largest contributor is the \"num_likes\", which makes sense from the exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the best labelling, from DBSCAN (although similar to agglom)\n",
    "\n",
    "best_labels_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new frame to analyse\n",
    "posts_counts = posts[columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign colum to the groupings\n",
    "posts_counts[\"cluster\"] = best_labels_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_counts.groupby(\"cluster\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The clusterings found have what appears to be zero likes\n",
    "posts_counts.groupby(\"cluster\")[\"num_likes\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a summary feel for the different clusters\n",
    "posts_counts.groupby(\"cluster\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that of the three groups (non-cluster counted as a group in this case), the groups are:\n",
    "\n",
    "* non-clustered: posts with no likes, but some other reactions (there are five points remember)\n",
    "* cluster 0: posts with a mix of likes and other reactions\n",
    "* cluster 1: posts with no likes or reactions\n",
    "\n",
    "Considering we have not transformed these features at all other than normalisation, this result is quite good! We can group different types of interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
