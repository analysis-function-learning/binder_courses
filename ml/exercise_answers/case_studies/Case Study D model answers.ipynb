{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1><font size=7> Case Study D</font> </h1> </center>\n",
    "\n",
    "# Exploring Covid US Government Loans - Example Answer\n",
    "\n",
    "This notebook contains a minimum example to complete the tasks in Case Study D. There are other approaches that may work - this is just one example approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.read_csv(\"../../data/NMLoans.csv\")\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explore the data to understand the different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"LoanAmount\"].plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a X_num using \"LoanAmount\", \"JobsReported\" and \"DaysApprovedSinceMay1st\", standard scale these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = loans[[\"LoanAmount\", \"JobsReported\", \"DaysApprovedSinceMay1st\"]]\n",
    "numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be useful alter for interpreting arrays\n",
    "numeric_names = numeric.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = numeric.to_numpy()\n",
    "X_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Perform PCA on X_num with only 1 component. Which feature contributes to this component the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_PCA = PCA(n_components=1).fit(X_num)\n",
    "\n",
    "num_PCA.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result shows that the first feature, \"LoansAmount\" contributes in degrees of mangnitude more than the other features to the first principle component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Explore the distributions of, then one-hot encode \"BusinessType\", \"RaceEthnicity\", \"Gender\", \"Veteran\", \"NonProfit\" into X_cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = loans[[\"BusinessType\", \"RaceEthnicity\", \"Gender\", \"Veteran\", \"NonProfit\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = categorical.columns.to_list()\n",
    "\n",
    "for index, column in enumerate(columns):\n",
    "    plt.figure()\n",
    "    plt.bar(x=categorical[column].value_counts().index, height=categorical[column].value_counts())\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant parts of the data are unanswered, we will treat this as a category within itself rather than missing and needing imputing for now. We do have some responses for these categories so will be able to infer some things, but not necessarily rigorously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not yet going to drop categories to handle multicolinearity\n",
    "ohencoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# The output is originally sparse, it can be easier to view / handle dense data\n",
    "X_cat = ohencoder.fit_transform(categorical).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the order of the features in the array\n",
    "category_names = ohencoder.get_feature_names().tolist()\n",
    "\n",
    "category_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Perform PCA on X_cat, explore the resulting components, which are the most important 5 features in the first component?. How many components would you choose to explain 70% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the categorical features\n",
    "cat_PCA = PCA().fit(X_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the first component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring what is important in the first component\n",
    "first_component = cat_PCA.components_[0].tolist()\n",
    "\n",
    "# We don't care if the components impact is positive or negative, just it's magnitude\n",
    "first_component_absolute = np.absolute(first_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the component's corresponding feature names\n",
    "names_important = list(zip(category_names, first_component_absolute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_important_sorted = sorted(names_important, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "names_important_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind ourselves of the feature groupings\n",
    "column_indexes = list(enumerate(columns))\n",
    "\n",
    "# get the prefixes\n",
    "prefixes = [(\"x\"+str(column_index[0]), column_index[1]) for column_index in column_indexes]\n",
    "\n",
    "prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 most important to this component\n",
    "names_important_sorted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the majority class, Unanswered, for a range of classes is impactful on the first component. Interestingly Male Owned response for the Gender is a significant projection within this component too.\n",
    "\n",
    "Determining the right number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evr_cumsum = cat_PCA.explained_variance_ratio_.cumsum()\n",
    "\n",
    "component_numbers = list(range(1, cat_PCA.n_components_ + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(component_numbers, evr_cumsum)\n",
    "plt.ylim(0, 1.2)\n",
    "plt.title(\"Cumulative sum of variance explained ratios across components\")\n",
    "plt.axhline(y=0.7, color='r', linestyle='--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first index where the cumsum is > 0.7\n",
    "# getting the value out of the 2D array requires multiple indexing\n",
    "first_index = np.argwhere(evr_cumsum > 0.7)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the component numbers to find which component this index corresponds to\n",
    "corresponding_component = component_numbers[first_index]\n",
    "\n",
    "corresponding_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 5 components explain the variance of 70% of the features. There are 29 features, showing we can compress out data quite well with few components as shown by the above figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Combine `X_num` with `X_cat` to make `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two data sets need to be combined rowwise, \n",
    "# this means the number of records should stay the same, \n",
    "# but have more columns\n",
    "\n",
    "X = np.concatenate((X_num, X_cat), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_num shape:\\t\", X_num.shape)\n",
    "print(\"X_cat shape:\\t\", X_cat.shape)\n",
    "print(\"X shape:\\t\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine our lists of names to interpret the resulting X array\n",
    "feature_names = numeric_names + category_names\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Remove the column for LoanAmount from X. Use TSNE to reduce the dimensions of X to two. Take a sample of 500 records in X if this methods takes a prohibitive amount of time. Plot the data using the \"LoansAmount\" as colour. Can you see a trend based on this projection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first column for array and supplementary names\n",
    "\n",
    "y = X[:,0]\n",
    "\n",
    "X = X[:,1:]\n",
    "\n",
    "feature_names = feature_names[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample to avoid excessive computation\n",
    "tsne_mask = np.random.randint(X.shape[0], size=500)\n",
    "\n",
    "X_sample = X[tsne_mask, :]\n",
    "\n",
    "y_sample = y[tsne_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Produce the learned reduced dimension data\n",
    "X_red = TSNE(n_components=2, n_jobs=-1).fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_red[:,0], X_red[:,1], c=y_sample.tolist(), alpha=0.5)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting visualisation will be different dependent on our random state due to how TSNE works with probabilities. From my running of this, there is not a clear / consistent trend between the resulting projections and the LoanAmount. However, there is some clustering of the data that occurs which may tell us that there are groupings of loans in some way based on the features given.\n",
    "\n",
    "Reminder: we don't want to interpret results produced from methods we cannot ourselves explain. t-SNE is more of a black box than some other methods for dimension reduction, but it can tell us about the similarity of some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Task\n",
    "\n",
    "We are going to encode and process our data from scratch to ensure reproducibility, using a column encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a train/test split within pandas to avoid leakage before we fit anything\n",
    "np.random.seed(1)\n",
    "\n",
    "# 0.8 for 80% split\n",
    "# Produce array of True/False values\n",
    "# With approx 80% True\n",
    "mask = np.random.rand(len(loans)) < 0.8\n",
    "\n",
    "# True values to train\n",
    "train = loans[mask]\n",
    "\n",
    "# False values to test\n",
    "test = loans[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\", train.shape)\n",
    "print(\"Test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the targer\n",
    "X_train, y_train = train.drop(columns=\"LoanAmount\"), train[[\"LoanAmount\"]]\n",
    "\n",
    "X_test, y_test = test.drop(columns=\"LoanAmount\"), test[[\"LoanAmount\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Transformer for all columns and processing\n",
    "# Remove unspecified columns with dropping\n",
    "column_trans = ColumnTransformer(\n",
    "    [('numeric', StandardScaler(), [\"JobsReported\", \"DaysApprovedSinceMay1st\"]), # numerical data scaled\n",
    "    ('categorical', OneHotEncoder(handle_unknown=\"ignore\", sparse=False), \n",
    "                 [\"BusinessType\", \"RaceEthnicity\", \"Gender\", \"Veteran\", \"NonProfit\"])], # categorical data onehot encoded\n",
    "    remainder=\"drop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Transformation\n",
    "X_train = column_trans.fit_transform(X_train)\n",
    "\n",
    "# Transform but not fit on test set\n",
    "# We do not want the test data to be learned,\n",
    "# Therefore we should not fit the data\n",
    "X_test = column_trans.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on X_train\n",
    "# by not specifying n_components we get all resulting components\n",
    "pca = PCA().fit(X_train)\n",
    "\n",
    "# Apply learned transformation to training data\n",
    "X_train_red = pca.transform(X_train)\n",
    "\n",
    "# Use the same learned transformation (projection) on test data\n",
    "X_test_red = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Finding minimum components to achieve rmse<30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through number of components and evaluate model\n",
    "# we need as many components are there are features\n",
    "components = list(range(1, X_train.shape[1] + 1))\n",
    "\n",
    "# store resulting values\n",
    "rmses = []\n",
    "\n",
    "for n in components:\n",
    "    \n",
    "    # Train model on the first n components \n",
    "    # Remember components are ordered by how important they are\n",
    "    lr = LinearRegression().fit(X_train_red[:,:n], y_train)\n",
    "    \n",
    "    y_pred = lr.predict(X_test_red[:,:n])\n",
    "    \n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    rmses.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(components, rmses)\n",
    "plt.axhline(y=30000, color='r', linestyle='--')\n",
    "plt.title(\"RMSE values across principal components used to train linear model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first below 30,000\n",
    "component_scores = zip(components, rmses)\n",
    "\n",
    "threshold_component, threshold_score = None, None\n",
    "\n",
    "# Assumes scores are ordered in descending order\n",
    "for component, score in component_scores:\n",
    "    if score < 30000:\n",
    "        threshold_component, threshold_score = component, score\n",
    "        break\n",
    "        \n",
    "print(\"Minimum number of components needed:\", threshold_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Gender breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can join the predictions onto the original data frame and perform aggregate scoring\n",
    "test_breakdowns = test.copy() # removes inplace warning\n",
    "\n",
    "# Train on previously found component number\n",
    "lr = LinearRegression().fit(X_train_red[:,:6], y_train)\n",
    "\n",
    "# generate predictions for the test set\n",
    "y_pred = lr.predict(X_test_red[:,:6])\n",
    "\n",
    "test_breakdowns[\"prediction\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could do this via loops, but vectorized can be more efficient\n",
    "unbalanced_breakdown = (test_breakdowns\n",
    "                        .groupby(\"Gender\")\n",
    "                        .apply(lambda x : pd.Series({\"count\": len(x), \n",
    "                                                     \"rmse\": mean_squared_error(x[\"LoanAmount\"], \n",
    "                                                                                x[\"prediction\"], \n",
    "                                                                                squared=False)})))\n",
    "unbalanced_breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we want low rmse value for good prediction.\n",
    "\n",
    "This tells us that our model performs worse for Female Owned buisnesses than for Male Owned. Be sure to look at the count of our breakdowns. Without them we can easily misinterpret aggregate data.\n",
    "\n",
    "The result could be a result of multiple effects, including but not limited to:\n",
    "\n",
    "* Unbalanced training data, more examples of Unanswered and Male Owned records will skew model weights to better predict those categories\n",
    "* Missing data effects, as a large portion of our data is missing \"Unanswered\" there may be covariances with Gender which make it harder to predict loans of one gender than another.\n",
    "* Statistical variance, the result may be random, performing hypothesis testing could help us determine whether this is likely. We can further explore the distributions of the errors produced. Considering we have taken just one sample - a train-test split the specifics of the data we picked may be harder to predict across Gender values\n",
    "* Bias, the features given to the model may better predict based on Gender of owner.\n",
    "\n",
    "Note: this is not an evaluation of the difference in loans received across genders, but rather an evaluation of how our model performs across this split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension\n",
    "\n",
    "This extension is left for an exercise for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Some techniques you may wish to consider to improve the performance of the model, potentially reducing the number of components required:\n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "* Look at combinations of features\n",
    "* Explore polynomial / higher order relationships\n",
    "* Combine One-hot encodings, reducing the number of categories\n",
    "* Explore different standardising methods\n",
    "* Transform data, such as converting to a log scale\n",
    "* Explore feature importance\n",
    "* Explore whether an order can be found for some of the categorical data\n",
    "\n",
    "#### Enhancing Data\n",
    "\n",
    "* Rebalance the data used to train the model across different features\n",
    "* Impute \"Unanswered\" responses\n",
    "* Remove outliers if they exist from the training set\n",
    "* Remove low value columns, reducing noise in components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
