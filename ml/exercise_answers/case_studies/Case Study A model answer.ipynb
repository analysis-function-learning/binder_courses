{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1><font size=7> Case Study A</font> </h1> </center>\n",
    "\n",
    "## Bank Churn - example answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file path\n",
    "churn_filepath = '../../data/churn.csv'\n",
    "\n",
    "# Import data into a data frame.\n",
    "raw_data = pd.read_csv(filepath_or_buffer=churn_filepath, delimiter=\",\")\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the data types.\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a train-test split as early as possible to avoid biasing our model.\n",
    "\n",
    "In the course previously we have done this quite late, but it's good practice to do it as soon as you can.\n",
    "\n",
    "Remember, we need to follow the same steps for processing the training and test data. So for any value found using the training data, such as a mean to impute a column, we are going to save it then use it later on for the test data.\n",
    "\n",
    "This process can be a bit cumbersome when using data frames (shown in this case study). Pipeline and composite transformers can be really useful in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train, raw_test = train_test_split(raw_data, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "raw_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Customer_Age\", \"Card_Category\" and \"Credit_Limits\" are the only columns with missing values. We wille explore each and impute the missing values with a sensible replacement. We are going to replace using basic summary statistics in a single variable, but we could use more complex methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Customer Age\n",
    "raw_train[\"Customer_Age\"].plot.density();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The data appears to be normally distributed, so a mean value will be sufficient\n",
    "# ~10% of this columns values are missing, so this will skew the data\n",
    "# centrally but we can ignore that for now.\n",
    "clean_train = raw_train.copy()\n",
    "\n",
    "median_age = raw_train[\"Customer_Age\"].median()\n",
    "\n",
    "clean_train[\"Customer_Age\"] = clean_train[\"Customer_Age\"].fillna(median_age)\n",
    "\n",
    "# Some skewing occurs centrally\n",
    "clean_train[\"Customer_Age\"].plot.density();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Card Category\n",
    "clean_train[\"Card_Category\"].value_counts(normalize=False).plot(kind=\"bar\"); # excludes missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the counts above the most common card category \"Blue\" is by far more common than any of the others, it is therefore a basic good choice for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From inspection, could/should be done programatically\n",
    "common_category = \"Blue\"\n",
    "\n",
    "# fill with most common category\n",
    "clean_train[\"Card_Category\"] = clean_train[\"Card_Category\"].fillna(common_category)\n",
    "\n",
    "# look at result\n",
    "clean_train[\"Card_Category\"].value_counts(normalize=False).plot(kind=\"bar\"); # excludes missing values\n",
    "clean_train[\"Card_Category\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the customer IDs are going to be unique and therefore a bad feature to include in the model. We will therefore remove them from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to impute the final missing data column, \"Credit_Limit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is skewed, but with a second peak between 30-40,000\n",
    "clean_train[\"Credit_Limit\"].plot.density();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we will ignore the second peak and impute with the median\n",
    "median_limit = clean_train[\"Credit_Limit\"].median()\n",
    "\n",
    "clean_train[\"Credit_Limit\"] = clean_train[\"Credit_Limit\"].fillna(median_limit)\n",
    "\n",
    "clean_train[\"Credit_Limit\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove unique columns.\n",
    "basic_feature_data = clean_train.drop(columns=[\"CLIENTNUM\"])\n",
    "basic_feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that the \"Income_Category\" data can be cleaned and encoded more appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_feature_data[\"Income_Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a number of options on how to encode this. We could split the values into two columns, with an upper and lower bound. We could encode the data as integers 0, 1, 2 expressing the order of the categories. We could one hot encode the values assuming that each is independent.\n",
    "\n",
    "As there is a clear order between the different levels I am going to encode each range with the mid point (40 - 60 -> 50), or the lower bound. The Unknown values will need to be imputed.\n",
    "\n",
    "We cannot perfectly encapsulate the interval nature of this data, as the intervals themselves are not well bounded, we can however approximate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_map = {\n",
    "    \"Less than $40K\": 20,\n",
    "    \"$40K - $60K\": 50,\n",
    "    \"$80K - $120K\": 100,\n",
    "    \"$120K +\": 120\n",
    "}\n",
    "\n",
    "# this will convert \"Unknown\" to NaN as it does not appear in the map dict\n",
    "basic_feature_data[\"Income_Category_Encoded\"] = basic_feature_data[\"Income_Category\"].map(income_map)\n",
    "basic_feature_data[\"Income_Category_Encoded\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is skewed, therefore use median\n",
    "median_income = basic_feature_data[\"Income_Category_Encoded\"].median()\n",
    "\n",
    "basic_feature_data[\"Income_Category_Encoded\"] = basic_feature_data[\"Income_Category_Encoded\"].fillna(median_income)\n",
    "\n",
    "basic_feature_data = basic_feature_data.drop(columns=[\"Income_Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Education_Level\" too is an ordinal data type. We have the choice to either encode independence into the data set (one hot encode), or to enforce an interval between categories (integer / label encoding). I am going to opt for interval encoding here, as the order is quite important to education level, and will give more insight. This could be tested and the methods compared. We will again, need to impute the unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore distribution\n",
    "basic_feature_data[\"Education_Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could quite likely group these together\n",
    "# but for now we will keep the separate\n",
    "education_map = {\n",
    "    \"Uneducated\": 1,\n",
    "    \"High School\": 2,\n",
    "    \"College\": 3,\n",
    "    \"Graduate\": 4, # assuming graduate is after college in US \n",
    "    \"Post-Graduate\": 5,\n",
    "    \"Doctorate\": 6\n",
    "}\n",
    "\n",
    "# this will convert \"Unknown\" to NaN as it does not appear in the map dict\n",
    "basic_feature_data[\"Education_Level_Encoded\"] = basic_feature_data[\"Education_Level\"].map(education_map)\n",
    "basic_feature_data[\"Education_Level_Encoded\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_feature_data[\"Education_Level_Encoded\"].value_counts().sort_index().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an interesting choice to make between the three averages based\n",
    "# on the plot above\n",
    "# the median and mean select an uncommon value\n",
    "print(\"median\", basic_feature_data[\"Education_Level_Encoded\"].median())\n",
    "print(\"mode\", basic_feature_data[\"Education_Level_Encoded\"].mode()[0])\n",
    "print(\"mean\", basic_feature_data[\"Education_Level_Encoded\"].mean())\n",
    "# I am going to select the mode here as it appears far more frequently than \n",
    "# the others and isn't significantly far from the others\n",
    "# This is one of the challenges with encoding intervals that may not be representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode produces a series, which needs to be indexed with [0] to get the value\n",
    "mode_education = basic_feature_data[\"Education_Level_Encoded\"].mode()[0]\n",
    "\n",
    "basic_feature_data[\"Education_Level_Encoded\"] = basic_feature_data[\"Education_Level_Encoded\"].fillna(mode_education)\n",
    "\n",
    "basic_feature_data = basic_feature_data.drop(columns=[\"Education_Level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the target class distribution of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the amount of each target class is within the data.\n",
    "basic_feature_data[\"Attrition_Flag\"].value_counts(normalize=True).plot(kind=\"bar\",\n",
    "                                                    color=[\"navy\", \"gold\"],\n",
    "                                                    title=\"Target: Attrition Flag distribution\");\n",
    "\n",
    "print(basic_feature_data[\"Attrition_Flag\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution may reduce the performance of our model, we are going to resample our data so that we have the same amount for both the classes. This will be achieved by undersampling the majority class (\"No\"/0), we have a reasonable number of minority class (342 samples) and therefore although we will lose some predictive power for \"No\", our model will generalise better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode to 1's and 0's\n",
    "target_map = {\n",
    "    \"Existing Customer\": 0,\n",
    "    \"Attrited Customer\": 1\n",
    "}\n",
    "basic_feature_data[\"Attrition_Flag_Encoded\"] = basic_feature_data[\"Attrition_Flag\"].map(target_map)\n",
    "\n",
    "# remove non-encoded column\n",
    "basic_feature_data = basic_feature_data.drop(columns=[\"Attrition_Flag\"])\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = basic_feature_data[basic_feature_data[\"Attrition_Flag_Encoded\"]==0]\n",
    "df_minority = basic_feature_data[basic_feature_data[\"Attrition_Flag_Encoded\"]==1]\n",
    " \n",
    "# Undersample majority class.\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,    # sample without replacement\n",
    "                                   n_samples=len(df_minority),    # to match minority class\n",
    "                                   random_state=123) # reproducible results\n",
    "\n",
    "\n",
    "basic_feature_resampled = pd.concat([df_majority_downsampled, df_minority], axis=0, sort=True)\n",
    "\n",
    "basic_feature_resampled = basic_feature_resampled.reset_index(drop=True)\n",
    "\n",
    "# Display new class counts\n",
    "basic_feature_resampled[\"Attrition_Flag_Encoded\"].value_counts()\n",
    "# expected_result (ish):\n",
    "#1    1316\n",
    "#0    1316\n",
    "#Name: Attrition_Flag, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_feature_resampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could alternatively do a `.groupby` followed by a `.sample`, however the above method works for arrays and dataframes.\n",
    "\n",
    "### Further Categorical Encoding\n",
    "\n",
    "We will need to encode the remaining categorical data: \"Card_Category\", \"Marital_Status\" and \"Gender\".\n",
    "\n",
    "Gender (in this data set, not in real life) is binary, which leads itself to 0, 1 encoding.\n",
    "\n",
    "Card Categories have some associated rank, and therefore be ordinal, however without the domain knowledge we will assume they are independent.\n",
    "\n",
    "Similarly there could be a progression of maritial status', however, we will assume they are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_feature_resampled.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly map the \"Gender\" feature to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map = {\"M\": 0, \"F\": 1}\n",
    "\n",
    "# Map gender values to 0 and 1, fill missing / unknown with majority class\n",
    "basic_feature_resampled[\"Gender\"] = basic_feature_resampled[\"Gender\"].map(gender_map)\n",
    "\n",
    "# calculate most common\n",
    "gender_mode = basic_feature_resampled[\"Gender\"].mode()\n",
    "\n",
    "basic_feature_resampled[\"Gender\"] = basic_feature_resampled[\"Gender\"].fillna(gender_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode the other two column's at once below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the encoder\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# We will keep the \"Unknown\" value for marital status for now\n",
    "\n",
    "# Make array of marital status and card category\n",
    "marital_card = one_hot_encoder.fit_transform(basic_feature_resampled[[\"Card_Category\", \"Marital_Status\"]]).toarray()\n",
    "\n",
    "# Store the different categories\n",
    "column_names = one_hot_encoder.get_feature_names()\n",
    "\n",
    "# Create a new data frame with the marital status and card categories data.\n",
    "marital_card_frame = pd.DataFrame(data=marital_card, columns=column_names)\n",
    "\n",
    "marital_card_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the data back to the training data, dropping the original columns\n",
    "basic_feature_resampled = basic_feature_resampled.drop(columns=[\"Marital_Status\", \"Card_Category\"])\n",
    "\n",
    "# concat (ensuring no rows have been dropped)\n",
    "basic_feature_resampled_encoded = pd.concat([basic_feature_resampled, marital_card_frame], axis=1)\n",
    "\n",
    "basic_feature_resampled_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets separate our target variable from our features and add our new \"Sex\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign target to a separate object.\n",
    "y_train = basic_feature_resampled_encoded[\"Attrition_Flag_Encoded\"]\n",
    "\n",
    "# remove target\n",
    "X_train = basic_feature_resampled_encoded.drop(columns=[\"Attrition_Flag_Encoded\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to scale our data. Because we are using a model which uses a distance metric it is important to use normalisation scaling so that all features are comparable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise robust scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the data with the normalizer.\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X=X_train), \n",
    "                              columns=[X_train.columns])\n",
    "\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other options to standardize the data, we are at first going to just look at normalized data, but we could use:\n",
    "\n",
    "* Only normalized\n",
    "* Standardized then normalized\n",
    "* Neither normalized or standardized\n",
    "* Only standardized (what we are going with)s\n",
    "\n",
    "Our data has already been train-test split, but we have only processed the training data, we need to do the same for the test frames before we evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the same steps from the training data processing\n",
    "# Reset index to stop dropping indexes being an issue\n",
    "clean_test = raw_test.copy().reset_index(drop=True)\n",
    "\n",
    "# Remove unique column\n",
    "clean_test = clean_test.drop(columns=\"CLIENTNUM\")\n",
    "\n",
    "# impute missing data\n",
    "clean_test[\"Customer_Age\"] = clean_test[\"Customer_Age\"].fillna(median_age)\n",
    "clean_test[\"Card_Category\"] = clean_test[\"Card_Category\"].fillna(common_category)\n",
    "clean_test[\"Credit_Limit\"] = clean_test[\"Credit_Limit\"].fillna(median_limit)\n",
    "\n",
    "# encode ordinal data\n",
    "clean_test[\"Income_Category\"] = clean_test[\"Income_Category\"].map(income_map)\n",
    "clean_test[\"Income_Category\"] = clean_test[\"Income_Category\"].fillna(median_income)\n",
    "\n",
    "clean_test[\"Education_Level\"] = clean_test[\"Education_Level\"].map(education_map)\n",
    "clean_test[\"Education_Level\"] = clean_test[\"Education_Level\"].fillna(mode_education)\n",
    "\n",
    "# one hot encode data\n",
    "clean_test[\"Gender\"] = clean_test[\"Gender\"].map(gender_map)\n",
    "clean_test[\"Gender\"] = clean_test[\"Gender\"].fillna(gender_mode)\n",
    "\n",
    "# **transform** not fit\n",
    "marital_card_test = one_hot_encoder.transform(clean_test[[\"Card_Category\", \"Marital_Status\"]]).toarray()\n",
    "marital_card_frame_test = pd.DataFrame(data=marital_card_test, columns=column_names)\n",
    "clean_test = clean_test.drop(columns=[\"Marital_Status\", \"Card_Category\"])\n",
    "\n",
    "# concat\n",
    "clean_test = pd.concat([clean_test, marital_card_frame_test], axis=1)\n",
    "\n",
    "# encode target\n",
    "clean_test[\"Attrition_Flag\"] = clean_test[\"Attrition_Flag\"].map(target_map)\n",
    "\n",
    "clean_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y for the test set\n",
    "y_test = clean_test[\"Attrition_Flag\"]\n",
    "X_test = clean_test.drop(columns=[\"Attrition_Flag\"])\n",
    "\n",
    "# Scale\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first train and evaluate a model using the single nearest neighbour to classify the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the classifier object\n",
    "neighbour_initial_model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Fit the model to the training data.\n",
    "neighbour_initial_model = neighbour_initial_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict values on the test set using the trained model.\n",
    "init_y_pred = neighbour_initial_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the names for the classification report to produce.\n",
    "target_names = target_map.keys()\n",
    "\n",
    "# Generate the report using the target test and prediction values.\n",
    "classif_report = classification_report(y_test, init_y_pred, target_names=target_names)\n",
    "\n",
    "print(classif_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good first attempt, but we could improve the F1 score probably by selecting a better K or weighting method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters and the values we want to search.\n",
    "parameters = {\"n_neighbors\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "              \"weights\": [\"uniform\", \"distance\"]}\n",
    "\n",
    "# Select the model type we have chosen.\n",
    "neighbour_improved_model = KNeighborsClassifier()\n",
    "\n",
    "# Set the number of folds we want to have to get 80:20 train/test split within the grid search cross validation.\n",
    "n_cv = 5\n",
    "\n",
    "# Define our grid search model to find optimal parameters.\n",
    "opt_model = GridSearchCV(estimator=neighbour_improved_model, param_grid=parameters, scoring=\"f1\", cv=n_cv)\n",
    "\n",
    "# Fit our parameter search model.\n",
    "opt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nThe best parameters found are: \\n\\n\", opt_model.best_params_)\n",
    "\n",
    "# Predict target values based on best model found.\n",
    "better_y_pred = opt_model.best_estimator_.predict(X_test_scaled)\n",
    "\n",
    "# Generate the report using the target test and predicted values.\n",
    "classif_report_new = classification_report(y_test, better_y_pred, target_names=target_names)\n",
    "\n",
    "print(classif_report_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this tuning hasn't substantiall improved the performance of the model. The macro average f1-score has increased by 1, with other metrics largely the same compared to $k=1$. However, the recall and f1 for Attrited Customers has improved. We will likely care about these values more than others as we want to be able to predict whether or not someone will leave our bank at any point, rather than predicting they will stay (most people stay at any point!).  \n",
    "\n",
    "Remember however, we have used some preprocessing steps which may not be optimal, we should explore whether we can improve the model by using different processes.\n",
    "\n",
    "Consider:\n",
    "\n",
    "* Imputing missing values differently (or drop)\n",
    "* Encoding ordinal data in a different manner\n",
    "* Resampling differently (the did the even sample even improve the model?)\n",
    "* Scaling the data differently (could we normalize the distances after scaling?)\n",
    "* Using a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
