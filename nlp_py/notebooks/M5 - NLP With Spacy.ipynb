{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Natural Language Processing With spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<br>\n",
    "<a href=\"#Module 4: Natural Language Processing With spaCy\"><font size=\"+1\">Module 4: Natural Language Processing With spaCy\n",
    "</font></a>\n",
    "<ol>\n",
    "  <li>What is spaCy?</li>\n",
    "  <li>Spacy - Tokenisation</li>\n",
    "  <li>Spacy - Checks</li>\n",
    "  <li>Rule-Based Matching using spaCy</li>\n",
    "  <li>Spacy - Stopwords</li>\n",
    "  <li>Spacy - Remove punctuation</li>\n",
    "  <li>Spacy - Remove Numbers</li>\n",
    "  <li>Spacy - Sentence Detection</li>\n",
    "  <li>Spacy - Lemmatization</li>\n",
    "  <li>Spacy - Part of Speech Tagging </li>\n",
    "  <li>Spacy - Shallow Parsing</li>\n",
    "  <li>Spacy - Named Entity Recognition</li>\n",
    "  <li>SpaCy - Dependency Parsing </li>  \n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the patents dataset to demonstrate how to undertake key NLP tasks using spaCy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Outcomes:** \n",
    "\n",
    "Perform the following operations on text using the spaCy library:-\n",
    "\n",
    "\n",
    "* Execute tokenisation\n",
    "* Remove stopwords\n",
    "* Remove punctuation\n",
    "* Remove numbers\n",
    "* Identify sentences\n",
    "* Execute Lemmatization\n",
    "* Execute Part of speech tagging\n",
    "* Execute Shallow Parsing (chunking)\n",
    "* Execute Named Entity Recognition\n",
    "* Execute Dependency Parsing\n",
    "\n",
    "Additionally you should be able to:\n",
    "\n",
    "* Provide a brief desciption of the spaCy library\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 What is spaCy?\n",
    "<br>\n",
    "\n",
    "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "spaCy is designed specifically for **production use** and helps you build applications that process and “understand” large volumes of text. It can be used to build **information extraction** or natural language understanding systems, or to **pre-process** text for deep learning.\n",
    "\n",
    "spaCy provides a variety of linguistic annotations to give you insights into a text’s grammatical structure. This includes the word types, like the parts of speech, and how the words are related to each other. For example, if you’re analyzing text, it makes a huge difference whether a noun is the subject of a sentence, or the object – or whether “google” is used as a verb, or refers to the website or company in a specific context. (from official spaCy documentation - link within the references section)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../pics/spacypipeline.svg\" alt=\"Summary of NLP pipeline in spaCy\" width=650>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "When you call nlp on a text, spaCy first tokenizes the text to produce a **Doc** object. The Doc is then processed in several different steps – this is also referred to as the **processing pipeline**. The pipeline used by the default models consists of a tagger, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between NLTK and spaCy\n",
    "\n",
    "We have until this point used `nltk` primarily, whereas in reality we will want to use the NLP toolkit that is most relevant for our task.\n",
    "\n",
    "`nltk` was originally a tool for academic research. It contains a wide range of algorithms that can be combined, customised and used with great flexibility. In this way using `nltk` is similar to starting from scratch. A greater degree of customisation is achievable.\n",
    "\n",
    "`spaCy` on the other hand is production oriented. Rather than giving a wide range of options, `spaCy` gives one, sometimes two, really good ways to solve an NLP problem.\n",
    "\n",
    "The choice between `nltk` and `spaCy` depends largely on what the project/analytical requirements are.\n",
    "\n",
    "* If developing new approaches, experimenting with techniques or learning about specific methods; `nltk` is likely your best bet.\n",
    "* If building an application of a well known task, wanting to focus on delivery, or just looking for out of the box high performance then `spaCy` can be really useful.\n",
    "\n",
    "`spaCy` will tokenize and perform other processing options requested for you by default, rather than having to specify approaches and build complicated process pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load in our practice data the same way we have in previous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe to process the abstract column\n",
    "patents = pd.read_pickle('../data/Patent_Dataset.pkl')\n",
    "\n",
    "# Fix index\n",
    "patents = patents.reset_index(drop=True)\n",
    "\n",
    "patents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course we will use a model stored within the folders. This model has been selected as it is basic and small yet an effective learning tool. To find more powerful models please see [the spaCy documentation](https://spacy.io/usage/models).\n",
    "\n",
    "You are welcome to install your own models to play around with, however, we will assume that you are using the local model loaded below. This model requires the version of `spacy` outlined in the pre-course instructions file.\n",
    "\n",
    "If you get an error loading the below, it is likely you have changed the directory structure intended, or do not have the correct version of `spacy` loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language model instance in spaCy locally\n",
    "nlp = spacy.load('../local_packages/spacy_local/small_practice_model/en_core_web_sm-3.1.0')\n",
    "\n",
    "# Create an nlp object using our text to analyse\n",
    "doc = nlp(\"Balo walked to school. She met several friends at the school gate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view the active pipeline components\n",
    "# these will be explored later\n",
    "# they tell us what we can do with our text in this pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Spacy - Tokenisation\n",
    "\n",
    "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to apply spacy processing to our data frame\n",
    "def token_spacy(pdoc):\n",
    "    pdoc = nlp(pdoc)\n",
    "    return [token.text for token in pdoc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patents['token_spacy'] = patents['abstract'].apply(token_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['token_spacy'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Spacy - Checks\n",
    "\n",
    "`spacy` comes with a variety of checks, allowing us to find out characteristics of the text we are looking at. These often return boolean values.\n",
    "\n",
    "<ul>\n",
    "    <li><b>idx</b> index of the string at which the token starts within the sentence, starting with 0</li>\n",
    "    <li><b>text_with_ws</b> prints token text with trailing space (if present)</li>\n",
    "    <li><b>is_alpha</b> detects if the token consists of alphabetic characters or not</li>\n",
    "    <li><b>is_punct</b> detects if the token is a punctuation symbol or not</li>\n",
    "    <li><b>is_space</b> detects if the token is a space or not.</li>\n",
    "    <li><b>shape</b> prints out the shape of the word</li>\n",
    "    <li><b>is_stop</b> detects if the token is a stop word or not.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the properties of each token\n",
    "text = nlp(\"There is a green hill far away. It is in a land I heard in a lullaby\")\n",
    "\n",
    "for token in text[:3]:\n",
    "    \n",
    "    print(token, # original token\n",
    "          token.idx, # token index number\n",
    "          token.text_with_ws, # original token with whitespace included\n",
    "          token.is_alpha, # boolean - is token alphabetical?\n",
    "          token.is_punct, # boolean - is token punctuation?\n",
    "          token.is_space, # boolean - is token a space (or multiple)?\n",
    "          token.shape_, # gives a size of the token, represented by x's\n",
    "          token.is_stop, # boolean - whether token is a stopword\n",
    "          \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the attributes and methods that can be used by a token object see the [spacy documentation](https://spacy.io/api/token#attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Rule-Based Matching using spaCy\n",
    "\n",
    "Find words and phrases in the text using user-defined rules. It is like Regular Expressions.\n",
    "\n",
    "Used to match patterns.\n",
    "You provide lists of dictionaries, one per token.\n",
    "\n",
    "The key in each dictionary is the type of attribute you would like to match, such as text, lowercasing or part of speech. The value in th dictionary is the actual string you want to match that has the attribute given. \n",
    "\n",
    "We wrap each pattern in a list, so we can have a list of patterns. This will be a list, that contains lists, which contain dictionaries.\n",
    "\n",
    "```{python}\n",
    "# Match exact token texts\n",
    "[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "# Match lexical attributes\n",
    "[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Match any token attributes\n",
    "[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]\n",
    "```\n",
    "\n",
    "\n",
    "spaCy POS tags  shown here: https://spacy.io/api/annotation\n",
    "\n",
    "\n",
    "Below, we are going to create our own rule based matcher.\n",
    "\n",
    "To do so we are going to create a matcher object, then add in patterns we want to match.\n",
    "\n",
    "The pattern we want to match is instances of \"iPhone X\", so we create a pattern saying we need a text token matching \"iPhone\" and a text token matchine \"X\" following it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load('../local_packages/spacy_local/small_practice_model/en_core_web_sm-3.1.0')\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]]\n",
    "matcher.add(\"IPHONE_PATTERN\", patterns=pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output has three elements. \n",
    "\n",
    "The first element, ‘9528407286733565721’, is the match ID, this identifies the specific match.\n",
    "\n",
    "The second and third elements are the positions of the matched tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to create a new matcher which finds full names from text.\n",
    "\n",
    "To do this we will define a function that creates the pattern needed, then scans the text, extracting the matches based on the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "text = nlp('Gus Proto is a Python developer currently working for a London-based Fintech company. \\\n",
    "            He is interested in learning Natural Language Processing.')\n",
    "\n",
    "\n",
    "\n",
    "def extract_full_name(nlp_doc):\n",
    "    \n",
    "    matcher = Matcher(nlp_doc.vocab)\n",
    "    \n",
    "    # Match a proper noun followed by a propernoun\n",
    "    name_pattern = [[{'POS': 'PROPN'}, {'POS': 'PROPN'}]]\n",
    "    matcher.add('FULL_NAME', patterns=name_pattern)\n",
    "    matches = matcher(nlp_doc)\n",
    "    allmatches = []\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        # slice the doc to only the range we want\n",
    "        span = nlp_doc[start:end]\n",
    "        allmatches.append(span)\n",
    "    \n",
    "    return allmatches\n",
    "\n",
    "full_names = extract_full_name(text)\n",
    "\n",
    "print(full_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the matcher on the doc\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    # slice the original text to get the wanted match\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**match_id**: hash value of the pattern name. This will be unique to each match, which means if we have many matches of the same text we can distinguish between each. Using the hash value we can go backwards and find the original match.\n",
    "\n",
    "**start**: start index of matched span\n",
    "\n",
    "**end**: end index of matched span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the two sentences below:\n",
    "\n",
    "*You can read this book* <br>\n",
    "*I will book my ticket* <br>\n",
    "<br>\n",
    "Does a sentence contains the word “book” in it or not. \n",
    "Looking to find the word “book” only if it has been used in the sentence as a noun.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"You read this book\")\n",
    "doc2 = nlp(\"I will book my ticket\")\n",
    "\n",
    "# book which is a noun\n",
    "pattern = [[{'TEXT': 'book', 'POS': 'NOUN'}]]\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# we need to add rules to the matcher\n",
    "# within the matcher object the arguments are:\n",
    "# \"rule_2\" - name of match, an ID\n",
    "# patterns - list of patterns\n",
    "matcher.add('rule_2', patterns=pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find matches within document 1\n",
    "matches = matcher(doc1)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the match\n",
    "doc1[matches[0][1]:matches[0][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find matches in document 2\n",
    "matches = matcher(doc2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parts of speech for each token\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first sentence above, “book” has been used as a noun and in the second sentence, it has been used as a verb. So, the spaCy matcher should be able to extract the pattern from the first sentence only. \n",
    "\n",
    "Below we will create a more complex pattern, seaching for adjective noun pairs, with an additional optional noun at the end. We can customize and make our patterns as complex as we like to capture the phenomena we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('../local_packages/spacy_local/small_practice_model/en_core_web_sm-3.1.0')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    "    )\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [[{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]]\n",
    "\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(key=\"ADJ_NOUN_PATTERN\", patterns=pattern,)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Spacy - Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are the most common words in a language, which have \"little semantic meaning\".\n",
    "Most sentences need to contain stop words in order to be full sentences that make sense.\n",
    "Stop words are removed because they aren’t significant and distort the word frequency analysis. \n",
    "spaCy has a list of stop words for the English language (Singh, 2019)\n",
    "\n",
    "Stopwords are always arbitrary, we choose which words are stop words. This means it is important to choose them ourselves, and see what words are being used by different packages. Below we look at the default `spacy` stopwords. We will likely want to specify our own stopwords in the future.\n",
    "\n",
    "If we only use default stopwords we will potentially remove text that is important to our task, or end up keeping unimportant words. [This stackoverflow example for updating a spacy stopword list may be useful](https://stackoverflow.com/a/51627002)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access built in stopwords\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Spacy has', len(spacy_stopwords), 'stopwords')\n",
    "\n",
    "# Display the first 10 stopwords spaCy has\n",
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to apply spacy stopword removal to pandas frame\n",
    "def remove_stopword_spacy(pdoc):\n",
    "    # takes a string as input, creates a document\n",
    "    pdoc = nlp(pdoc)\n",
    "    # iterate through tokens, keep them if they are not stop words\n",
    "    # join together resulting tokens with a space in between each\n",
    "    text = \" \".join([str(token) for token in pdoc if not token.is_stop])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply stopword removal\n",
    "patents['preprocess_spacy'] = patents['abstract'].apply(remove_stopword_spacy)\n",
    "\n",
    "# not our data has been tokenized, then rejoined as text\n",
    "removed_stopwords_example = patents['preprocess_spacy'][0]\n",
    "removed_stopwords_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Spacy - Remove punctuation\n",
    "\n",
    "Below is a function that uses `spacy`'s token characteristics to remove punctuation. Note that we are applying it to the already processed text, so the tokenisation may be different to the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_spacy(pdoc):\n",
    "    pdoc = nlp(pdoc)\n",
    "    text = \"\"\n",
    "    for token in pdoc:\n",
    "        if not token.is_punct:\n",
    "            # update text with next token\n",
    "            text = text + \" \" + token.text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['preprocess_spacy'] = patents['preprocess_spacy'].apply(punctuation_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of removed punctionation (and stopwords)\n",
    "removed_punctuation_example = patents['preprocess_spacy'][0]\n",
    "\n",
    "print(\"\\t\\tBefore punctuation removal:\\n\", removed_stopwords_example)\n",
    "print(\"\\t\\tAfter punctuation removal:\\n\", removed_punctuation_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Spacy - Remove Numbers\n",
    "\n",
    "Below we create a new function that only keeps alphabetical characters. As we have already removed punctuation, this step will remove numbers.\n",
    "\n",
    "We therefore are using the properties of the text, and what we know about it to achieve a desired outcome, rather than explicitly programming what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonumbers_spacy(pdoc):\n",
    "    pdoc = nlp(pdoc)\n",
    "    text = \"\"\n",
    "    for token in pdoc:\n",
    "        # keep only alpha\n",
    "        if token.is_alpha:\n",
    "            text = text + \" \" + token.text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['preprocess_spacy'] = patents['preprocess_spacy'].apply(nonumbers_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of removed numbers (and stopwords, punctuation)\n",
    "removed_numbers_example = patents['preprocess_spacy'][2]\n",
    "print(\"\\t\\tExample text with numbers:\\n\", patents['abstract'][2])\n",
    "print(\"\\t\\tExample text with numbers:\\n\", removed_numbers_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Spacy - Sentence Detection\n",
    "<br>\n",
    "\n",
    "Sentence Detection is the process of locating the start and end of sentences in a given text. This separates the text into linguistically meaningful units. In spaCy, the **sents** property is used to extract sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp(\"There is a green hill far away. It is in a land I heard in a lullaby\")\n",
    "sentences = list(text.sents)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence, \"\\n\\tType:\", type(sentence).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_spacy(pdoc):\n",
    "    pdoc = nlp(pdoc)\n",
    "    return list(pdoc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['sentence_spacy'] = patents['abstract'].apply(sentence_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['abstract'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text has been split into sentences\n",
    "patents['sentence_spacy'][1]#[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Spacy - Lemmatization\n",
    "\n",
    "\n",
    "**Lemmatization** is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form or root word is called a **lemma**.\n",
    "\n",
    "Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text. (Singh, 2019)\n",
    "\n",
    "Below we will lemmatize the text and rejoin each string together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_spacy(pdoc):\n",
    "    \n",
    "    pdoc =  nlp(pdoc)\n",
    "    text  = \"\"\n",
    "    \n",
    "    for token in pdoc:\n",
    "        text = text + \" \" + str(token.lemma_)\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['preprocess_spacy'] = patents['preprocess_spacy'].apply(lemmatization_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulting lemmatized text\n",
    "patents['preprocess_spacy'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Spacy - Part of Speech Tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence.\n",
    "<br>\n",
    "\n",
    "**Part of speech tagging** is the process of **assigning a POS tag**  to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word.\n",
    "<br>\n",
    "\n",
    "In spaCy, POS tags are available as an attribute on the Token object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the text below the \"\\\" symbol is a line break which allows us to write one string (or general code)\n",
    "# across multiple lines\n",
    "text = nlp(\"Algebra can essentially be considered as doing computations\\\n",
    "            similar to those of arithmetic but with non-numerical mathematical objects. \\\n",
    "            However, until the 19th century, algebra consisted essentially of the theory \\\n",
    "            of equations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in text[:10]:\n",
    "    print(f\"{token}, {token.tag_}, {token.pos_}, {spacy.explain(token.tag_)}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to use spact part of speech tagging with pandas\n",
    "def pos_spacy(pdoc):\n",
    "    \n",
    "    pdoc = nlp(pdoc)\n",
    "    pos = []\n",
    "    \n",
    "    for token in pdoc:\n",
    "        pos.append([token.text, \"-->\", token.pos_])\n",
    " \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['pos_spacy'] = patents['abstract'].apply(pos_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['pos_spacy'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11 Spacy - Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shallow parsing, or chunking**, is the process of extracting phrases from unstructured text. Chunking groups adjacent tokens into phrases on the basis of their POS tags. There are some standard well-known chunks such as **noun phrases, verb phrases, and prepositional phrases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun chunks example shown - to view example of verb phrase extraction see example at \n",
    "https://realpython.com/natural-language-processing-spacy-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below extracted from https://realpython.com/natural-language-processing-spacy-python/\n",
    "text = ('There is a developer conference happening on 21 July 2019 in London.')\n",
    "text = nlp(text)\n",
    "\n",
    "# Extract Noun Phrases\n",
    "for chunk in text.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nounchunk_spacy(pdoc):\n",
    "    \n",
    "    pdoc =  nlp(pdoc)\n",
    "    noun_chunks  = []\n",
    "    \n",
    "    for chunk in pdoc.noun_chunks:\n",
    "        noun_chunks.append(chunk)\n",
    "        \n",
    "    return noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['noun_chunks_spacy'] = patents['abstract'].apply(nounchunk_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['noun_chunks_spacy'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12 Spacy - Named Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Named Entity Recognition (NER)** is the process of locating named entities in unstructured text and then classifying them into pre-defined categories, such as person names, organizations, locations, monetary values, percentages, time expressions, and so on.\n",
    "\n",
    "\n",
    "spaCy has the property ents on Doc objects. You can use it to extract named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example taken from https://realpython.com/natural-language-processing-spacy-python/\n",
    "text = ('Great Piano Academy is situated in Mayfair or the City of London and has world-class piano instructors.')\n",
    "\n",
    "text = nlp(text)\n",
    "\n",
    "for entity in text.ents:\n",
    "    # show the text, start and end index, entity label and explaination of said label\n",
    "    print(entity.text, entity.start_char, entity.end_char, entity.label_, spacy.explain(entity.label_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ne_spacy(pdoc):\n",
    "    \n",
    "    pdoc =  nlp(pdoc)\n",
    "    named_entities  = []\n",
    "    \n",
    "    for entity in pdoc.ents:\n",
    "        named_entities.append([entity.text, \"--->\", entity.label_] )\n",
    "        \n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['ne_spacy'] = patents['abstract'].apply(ne_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results of named entity recognition\n",
    "# have a look at other documents\n",
    "patents['ne_spacy'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['abstract'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.13 spaCy - Dependency Parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependency parsing** is the process of extracting the dependency parse of a sentence to represent its grammatical structure. It defines the dependency relationship between **headwords and their dependents**. The head of a sentence has no dependency and is called the **root** of the sentence. The verb is usually the head of the sentence. All other words are linked to the headword.\n",
    "\n",
    "The dependencies can be mapped in a directed graph representation:\n",
    "<br>\n",
    "Words are the nodes.<br>\n",
    "The grammatical relationships are the edges.<br>\n",
    "Dependency parsing helps you know what role a word plays in the text and how different words relate to each other. It’s also used in shallow parsing and named entity recognition. (Singh,2019)\n",
    "<br>\n",
    "Here’s how you can use dependency parsing to see the relationships between words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy comes with a built-in visualizer called displaCy. You can use it to visualize a dependency parse or named entities in a browser or a Jupyter notebook.\n",
    "\n",
    "You can use displaCy to find POS tags for tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "about_interest_text = ('He is interested in learning natural Language Processing.')\n",
    "about_interest_doc = nlp(about_interest_text)\n",
    "# the below will launch a displacy visualisation. to continue hit the stop button\n",
    "# this may not run on all machines, depending on package and permissions\n",
    "displacy.render(about_interest_doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://realpython.com/natural-language-processing-spacy-python/\n",
    "text = 'Gus is learning piano'\n",
    "text = nlp(text)\n",
    "for token in text:\n",
    "    # show the text, it's corresponding tag, the where the head of that token \n",
    "    # is pointing to and it's dependency\n",
    "    print(token.text, token.tag_, token.head.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(text, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependency tag ROOT denotes the main verb or action in the sentence. <br>\n",
    "The other words are directly or indirectly connected to the ROOT word of the sentence. <br>\n",
    "You can find out what other tags stand for by executing the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"nsubj\"), spacy.explain(\"ROOT\"), spacy.explain(\"aux\"), spacy.explain(\"advcl\"), spacy.explain(\"dobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to show dependecies in pandas text\n",
    "def depend_parse_spacy(pdoc):\n",
    "    \n",
    "    pdoc =  nlp(pdoc)\n",
    "    de_parse  = []\n",
    "    \n",
    "    for token in pdoc:\n",
    "        de_parse.append([token.text, \"--->\", token.dep_])\n",
    "        \n",
    "    return de_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['de_parse_spacy'] = patents['abstract'].apply(depend_parse_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patents['de_parse_spacy'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.14 Processing Pipeline\n",
    "\n",
    "So far we have looked at individual functionalities of the techniques provided by `spaCy`. The power of `spaCy` comes in when we can use these steps one after another in a pipeline, one after another.\n",
    "\n",
    "When we load a model, this by default has certain steps in the enabled.\n",
    "\n",
    "Below we are going to explore some basic properties of the pipelines themselves and how to interact with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language model instance in spaCy locally\n",
    "nlp = spacy.load('../local_packages/spacy_local/small_practice_model/en_core_web_sm-3.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the steps in the pipeline loaded\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the specific objects used for each step\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can remove the Named Entity Recognition step\n",
    "nlp.remove_pipe(\"ner\")\n",
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using NER, we could use [merge_noun_chunks](https://spacy.io/api/pipeline-functions#merge_noun_chunks) instead. This can be added to the pipeline after the existing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline step\n",
    "merge_noun_chunks = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "# Add it to pipeline\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New step has been added\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract noun chunks where we could not with the original pipeline\n",
    "example_merged = nlp(\"I bought a blue car for my great grandad\")\n",
    "\n",
    "list(example_merged.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spaCy` pipelines are incredibly powerful, and already have pre-existing components for most NLP processing. \n",
    "\n",
    "For more information on what you can do with the pipelines [see the spaCy documentation on the topic](https://spacy.io/usage/processing-pipelines). You can add your own customized steps in a pipeline too if you wish. It is well worth a read to see what already exists before trying to impliment a process from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "<br>\n",
    "\n",
    "<ol>\n",
    "     <li>Import the Hep Dataset and  using spacy perform the steps listed below to the text column. Add new columns to hold         results of each operation</li>\n",
    "    \n",
    "            Tokenise\n",
    "            Identify all phrases in column that have the pattern - adjective/noun.\n",
    "            Remove all stopwords\n",
    "            Remove all punctuation\n",
    "            Remove all numbers\n",
    "            Identify sentences\n",
    "            Lemmatize the text\n",
    "            Apply POS tagging\n",
    "            Apply shallow parsing\n",
    "            Apply Named Entity Recognition\n",
    "            Apply Dependency Parsing\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "</ol>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/usage/spacy-101 <br>\n",
    "https://www.analyticsvidhya.com/blog/2020/03/spacy-tutorial-learn-natural-language-processing/ <br>\n",
    "https://realpython.com/natural-language-processing-spacy-python/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_grad",
   "language": "python",
   "name": "master_grad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
