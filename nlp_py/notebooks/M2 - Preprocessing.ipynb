{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Text Normalisation - Preprocessing\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<br>\n",
    "\n",
    "<a href=\"#Module 2: Text Normalisation - Preprocessing\"><font size=\"+1\">Module 2: Text Normalisation - Preprocessing</font></a>\n",
    "<ol>\n",
    "  <li>What is Text Normalisation?</li>\n",
    "  <li>Lowercasing</li>\n",
    "  <li>Remove Punctuation</li>\n",
    "  <li>Tokenize</li>\n",
    "  <li>Lemmatization</li>\n",
    "  <li>Stemming</li>\n",
    "  <li>Stopword Removal</li>\n",
    "  <li>Remove Numbers</li>\n",
    "  <li>Remove the words having length less than 2</li>\n",
    "  <li>Using Stanza (not available for ONS devices)</li>\n",
    "  <li>Challenges</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Outcomes:** \n",
    "\n",
    "* Explain the concept of normalisation, \n",
    "\n",
    "* Execute the following preprocessing steps to a dataset using nltk: \n",
    "\n",
    "    + Lowercase \n",
    "\n",
    "    + Tokenize \n",
    "\n",
    "    + Lemmatization \n",
    "    \n",
    "    + Stemming \n",
    "\n",
    "    + Removing stop words and punctuation\n",
    " \n",
    " \n",
    "*  Differentiate between lemmatization and stemming. \n",
    "<br>\n",
    "\n",
    "Additionally you should be able to:\n",
    "\n",
    "* Execute tokenisation and lemmatisation using the Stanford Stanza library\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries used in this module\n",
    "import nltk\n",
    "nltk.data.path.append(\"../local_packages/nltk_data\")\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of stanza is optional\n",
    "# it will not work on some locked down devices (such as ONS machines)\n",
    "#import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load in a data set that will be used to explore text processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data is stored in a pickle, a filetype that lets us store python objects\n",
    "patents = pd.read_pickle('../data/Patent_Dataset.pkl')\n",
    "\n",
    "# our index is incorrect, quickly reset it\n",
    "patents = patents.reset_index(drop=True)\n",
    "\n",
    "patents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is Text Preprocessing?\n",
    "\n",
    "<br>\n",
    "\n",
    "Pre-processing text means converting it to a more convenient, standard form used for your specific application. It aims to put all text on a level playing field. It requires being aware of what type of text is to be normalized and how it is to be processed afterwards; there is no all-purpose, one size fits all preprocessing procedure. \n",
    "<br>\n",
    "To normalise text some key preprocessing steps could be undertaken, for example:\n",
    "\n",
    "<ul>\n",
    "  <li>Lowercasing</li>\n",
    "  <li>Removing all irrelevant characters (Numbers and Punctuation)</li>\n",
    "  <li>Removing words that aren't useful to us - \"Stopwords\" </li>\n",
    "  <li>Stemming</li>\n",
    "  <li>Lemmatization</li>\n",
    "</ul>\n",
    "\n",
    "The above is often referred to as *cleaning* the text. It is also part of an NLP *pipeline*\n",
    "\n",
    "<br>\n",
    "Typically, the text also undergoes <strong>tokenisation</strong> (segmenting text into words) to aid the normalisation process.\n",
    "<br>\n",
    "\n",
    "Once through preprocessing your text becomes more predictable and analyzable.  \n",
    "\n",
    "The pre-processing steps undertaken depends on the task. It is not necessary to do any or all of the steps shown below.\n",
    "\n",
    "There are a range of terminology that are related and used in NLP pre-processing:\n",
    "\n",
    "* Pre-processing - the broad term for steps to make text ready for analysis\n",
    "* Text Normalisation - reducing the variance in text by converting similar words to the same word\n",
    "* Text Cleaning - removing unecessary parts of text such as irrelevant words or characters\n",
    "\n",
    "Text preprocessing choices are highly dependent on the application. Also included in pre-processing is feature representation which involves converting text to numerical represenation, this is covered within the \"Intermediate NLP\" course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lowercasing\n",
    "<br>\n",
    "Lowercasing all the text data is one of the simplest and most effective forms of text normalisation. \n",
    "\n",
    "Lowercasing is applicable to some NLP tasks and significantly helps with consistency of the output.\n",
    "\n",
    "It is useful because in a lot of applications where we are trying to capture meaning, capitalised words have the same meaning as those in lowercase. For example `Cat` (noun) at the beginning of a sentence is referring to the same thing as `cat` in the middle of a sentence.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the text before we lowercase it\n",
    "# using the column \"abstract\" from the data frame and\n",
    "# observing the first row in that column (index 0)\n",
    "patents[\"abstract\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the text using pandas string methods\n",
    "patents['abstract_lower'] = patents['abstract'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the text after lowercasing\n",
    "patents[\"abstract_lower\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: lowercasing can be really useful in reducing the difference in words in a text. However, this is not always what we want.\n",
    "\n",
    "Upper casing text can also be informative to use. For example - if we were looking to extract all \"Named Entities\" - such as people's names, locations, products or organisations, then having a capital letter could be really useful!\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "<i>\"A turkey may march in Turkey in May or March\"</i>\n",
    "\n",
    "\n",
    "In the above we will want to treat \"Turkey\" and \"turkey\" differently, as well as May/may and March/march.\n",
    "\n",
    "We decide whether to lowercase based on the task at hand, and whether doing so would help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove Punctuation\n",
    "<br>\n",
    "Punctuation is sometimes irrelevant to the task at hand (such as counting word frequencies). By removing all or certain punctuation we remove the 'quirks' of individual sentences, standardising the text.\n",
    "\n",
    "Python provides a constant called `string.punctuation` that provides a great list of punctuation characters. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove the punctuation from our pandas text data. To do this we need to know what the punctuation characters are. We then need a function that will remove punctuation from a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the standard string punctuation\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create a regular expression (covered in the next chapter)\n",
    "# which captures all the above punctuation characters\n",
    "\"[{}]\".format(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a function that uses regex to remove punctuation from strings\n",
    "def remove_punct(ptext):\n",
    "    # replace any punctuation with nothing \"\", effectively removing it\n",
    "    ptext = re.sub(string=ptext,\n",
    "                   pattern=\"[{}]\".format(string.punctuation), \n",
    "                   repl=\"\")\n",
    "    return ptext\n",
    "\n",
    "# by making a function that works for one piece of text\n",
    "# we can then apply the function to all the pandas text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing our text before removing punctuation\n",
    "patents[\"abstract\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply removing punctuation function to all elements in the column \"abstract\"\n",
    "patents['abstract_no_punct'] = patents['abstract'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['abstract_no_punct'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above there are no longer punctuation marks. This may make our text not make perfect gramatical sense!\n",
    "\n",
    "Removing punctuation is one approach to cleaning text. A broader approach is removing non-alphanumeric text. This would cover punctuation, numbers and any other text that is not letters.\n",
    "\n",
    "We can use basic approaches, like removing all text that isn't letters. Or we could use more advanced methods that take into account the relationships between words.\n",
    "\n",
    "For example, if we simply removed all non-alphanumeric text from `\"let's\"` we would get `\"lets\"` which has a different meaning to `\"let us\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Tokenize\n",
    "<br>\n",
    "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks and others can be considered as tokens. Tokenization makes it easier to process the text eg find its frequency. \n",
    "\n",
    "\n",
    "Consider the following text:\n",
    "\n",
    "`\"This is the text given\"`\n",
    "\n",
    "By tokenising the text we receive the following tokens:\n",
    "\n",
    "`\"This\", \"is\", \"the\" \"text\", \"given\"` \n",
    "\n",
    "In python this often means we go from a text string to a list of strings.\n",
    "\n",
    "Tokenizers can range in complexity, from using `.split(\" \")` on text, to more rule based or machine learning-based implementations. Below we use `nltk`'s word tokenizer.\n",
    "\n",
    "As some tokenisers can take into account punctuation we do not need to always remove punctuation.\n",
    "\n",
    "We are going to first look at a basic example of tokenization, then use a more advanced method from `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tokenization example\n",
    "original_text = \"This is the text given\"\n",
    "\n",
    "# Create tokens by splitting the text on each \" \" space\n",
    "tokens = original_text.split(\" \")\n",
    "\n",
    "print(\"Original text:\\n\\t\", original_text)\n",
    "print(\"Original text 'type':\\n\\t\", type(original_text).__name__)\n",
    "print(\"Tokenized text:\\n\\t\", tokens)\n",
    "print(\"Tokenized text 'type':\\n\\t\", type(tokens).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using split, we can tokenise using functions from nltk, \n",
    "# other libraries, or create our own\n",
    "# For our basic text example this will provide the same result\n",
    "# For more complex/larger text this will be better than splitting\n",
    "nltk_tokens = nltk.word_tokenize(original_text)\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text before tokenization\n",
    "patents[\"abstract\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to 'abstract' column in dataframe\n",
    "patents['abstract_tokens'] = patents['abstract'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents[\"abstract_tokens\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text can also be split to identify sentences.\n",
    "\n",
    "This can be done using the **sent_tokenize()** function from nltk.\n",
    "\n",
    "Think about what separates sentences, typically punctuation, this can be used to determine where sentences start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenisation to abstract column in dataframe\n",
    "patents['abstract_sentences'] = patents['abstract'].apply(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text before sentence segmentation\n",
    "patents[\"abstract\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text after sentence segmentation\n",
    "patents[\"abstract_sentences\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above abstract is one long sentence! Let's look at another abstract to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents[\"abstract_sentences\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go, each sentence has become it's own element in a list. After tokenizing the sentences we could then tokenize the individual words within the longer sentence string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Stemming\n",
    "<br>\n",
    "<strong> The process of reducing inflected (or sometimes derived) words to their word stem; that is, their base or root form.</strong>  \n",
    "\n",
    "<br>\n",
    "\n",
    "For example, the words; *argue, argued, argues, arguing* reduce to the stem *argu*. \n",
    "\n",
    "Usually stemming is a crude heuristic process that chops off the ends of words in the hope of achieving the root correctly most of the time.\n",
    "\n",
    "Stemming aims to remove the excess part of the word to be able to identify words that are similar.\n",
    "\n",
    "There are different algorithms that can be used in the stemming process, but the most common in English is <strong>Porter Stemmer.</strong>  The rules contained in this algorithm are divided into different phases.The purpose of these rules is to reduce the words to their root.\n",
    "\n",
    "The danger here lies in the possibility of overstemming where words like “universe” and “university” are reduced to the same root of “univers”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to look at the results of using a stemmer on text. The resulting strings will not necessarily be real words, but they will reduce the diversity of the words in the text. We apply stemming to each word in a text, which means each word needs to be separate from it's neighbours. This is done using tokenising, so we can take a string of words, break them up, then apply a process to each independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the PorterStemmer from nltk\n",
    "words_to_normalise = \"The Ones and twos argues who is winning in todays matches\"\n",
    "\n",
    "# generate tokens\n",
    "tokens = nltk.word_tokenize(words_to_normalise)\n",
    "\n",
    "# note we have not applied any other preprocessing to the text\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each token in the list and apply the stemming\n",
    "tokens_stemmed = [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "# A list of stemmed words\n",
    "print(tokens_stemmed)\n",
    "# The has clearly applied some processing to the text, beyond chopping\n",
    "# off an ending. This PorterStemmer has also lowercased the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this course we are going to follow a similar code structure to the bellow:\n",
    "\n",
    "* write a function that performs what we want on a single piece of data (such as a string, or list depending on context)\n",
    "* apply this function to every row in the data set\n",
    "\n",
    "There are often ways we can use pandas itself to do string manipulation which may be more efficient than creating a custom function, but for some data structures like lists to deal with the pandas solution is harder to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying stemming to the pandas data\n",
    "# Define stemming function\n",
    "\n",
    "def stemming(ptoken):\n",
    "    # create stemming object\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in ptoken]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens pre-stemming\n",
    "patents['abstract_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['abstract_tokens_stemmed'] = patents['abstract_tokens'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens post stemming\n",
    "patents['abstract_tokens_stemmed'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comparing the pre and post stemmed tokens\n",
    "list(zip(patents['abstract_tokens'][0], patents['abstract_tokens_stemmed'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at those results we can see that the number of different tokens will reduce in a big corpus. However, this does make some of the resulting tokens less informative. There's a tradeoff here between how understandable our resulting words are, and how much we reduce the diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Lemmatization\n",
    "<br>\n",
    "\n",
    "Lemmatisation uses vocabulary and morphological analysis of words to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the <strong> lemma.</strong>  Most lemmatisers achieve this using a lookup table and so this process, when you have large volumes of text may be slower than the alternative; stemming. However, it is often a recommended approach in a variety of applications due to it's accuracy.\n",
    "\n",
    "Also, sometimes, the same word can have multiple different ‘lemma’s. So, based on the context it’s used in, you should identify the ‘part-of-speech’ (POS) tag for the word in that specific context and extract the appropriate lemma. POS tagging will be discussed in more detail in later modules.\n",
    "\n",
    "A part of speech refers to the type of a word. For example we can have nouns, verb, adjectives and so on to describe a word. We can however go into much more depth to describe what a word part of speech is by introducing more and more types. Below a basic example is shown. When we know what part of speech a word is we can better lemmatize it.\n",
    "\n",
    "<img src=\"../pics/posTag.png\" alt=\"Example tagging\">\n",
    "\n",
    "The WordNet lemmatizer shown below is a \"lexical\" lemmatizer. It contains a wide range of words, their lemmas and when called takes whatever word is given, and returns the lemma if it has an entry for it. If it doesn't have an entry it will just return the origin word.\n",
    "\n",
    "<br>\n",
    "\n",
    "Bewlow we are going to follow a similar set of steps to the stemming above. We will analyse the effect of lemmatising on our toy example, then on the pandas patents data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the WordNetLemmatizer from nltk\n",
    "# without using the parts of speech tagging it doesn't perform incredibly well\n",
    "# But it does always result in valid words where it runs\n",
    "\n",
    "# Run tokenizer\n",
    "tokens = nltk.word_tokenize(words_to_normalise)\n",
    "print(words_to_normalise)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lemmed = [WordNetLemmatizer().lemmatize(token) for token in tokens]\n",
    "print(tokens_lemmed)\n",
    "type(tokens_lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the stemmer we have more words that have been missed / not touched. However, \"todays\" -> \"today\", \"matches\" -> \"match\" and so on, which are good normalisations. The words that have been lemmatized are now actual words, in comparison with the stemming method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lemmatize() function\n",
    "\n",
    "def lemmatise(ptokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in ptokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens before lemmatization\n",
    "patents[\"abstract_tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply lemmatisation to all tokens in column\n",
    "patents['abstract_tokens_lemmatised'] = patents['abstract_tokens'].apply(lemmatise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens after lemmatization\n",
    "patents[\"abstract_tokens_lemmatised\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of normalisation\n",
    "list(zip(patents[\"abstract_tokens\"][0], patents[\"abstract_tokens_lemmatised\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that in this case lemmatizing have not been particularly effective. Lemmatizing needs to understand more about the words just the words in isolation to be able to normalize them effectively. (If you look closely, \"tufts\" has changed to \"tuft\"). The reason for this poor performance is that the lemmatizer is assuming that every token is a Noun. There are fewer changes that need to be made to normalize nouns than for other parts of speech, such as verbs. For example a verb \"running\", should be converted to \"run\". But if the lemmatizer thinks \"running\" is a noun, then it will not have a lookup to check for it and therefore not change anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is a simple example of how to use the wordnet lemmatizer on words and sentences.\n",
    "Performance can be improved if we add the correct ‘part-of-speech’ tag (POS tag) as the second argument to lemmatize().\n",
    "An example of this will be provided later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Lemmatization and Stemming:** Stemming operates on each word without considering the context and it cannot discriminate between different word meaning. Lemmatization, however, takes into account the part of speech and the context. Stemming is explained in more detail below.\n",
    "\n",
    "Caring -> Lemmatization -> ‘Care’ \n",
    "<br>\n",
    "Caring -> Stemming -> ‘Car’\n",
    "\n",
    "\n",
    "**Example:**\n",
    " \n",
    "<i>\"better\"</i>: has <i>\"good\"</i> as its lemma and \"better\" as its stem <br>\n",
    "<i>\"walking\"</i>: has  <i>\"walk\"</i> as its lemma and stem <br>\n",
    "<i>\"meeting\"</i>: can be either the base a noun or a verb depending on the context, eg <i>\"in our last meeting\"</i> or <i>\"We are meeting again tomorrow\"</i>. \n",
    "<br>\n",
    "Lemmatization can select the appropriate lemma based on the context, unlike stemming.\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../pics/stemlemm.png\" alt=\"Comparison of stemming and lemmatization outcomes\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Stemming is often a faster process, but less accurate / useful. So the specific use case is important in deciding which method to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Stopword Removal\n",
    "\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”). Stop words can be filtered from the text to be processed. There is no universal list of stop words in NLP, however the nltk module contains a list of stop words. Removal of stopwords can boost performance in machine learning classification tasks.\n",
    "\n",
    "Stop word removal is commonly applied in search systems, text classification applications, topic modeling, topic extraction and others. Stop word lists can come from pre-established sets or you can create a custom one for your domain.\n",
    "\n",
    "Often we will need to select out own stopwords for a task, because not all text is the same and all language can be different. What is \"unimportant\" in one analysis/application may be crucial in another.\n",
    "\n",
    "The reason this often works is because stopwords often do not add meaning to the sentence. For example if you wanted to work out what was important in the sentence below:\n",
    "\n",
    "\"The cat ate a mouse\"\n",
    "\n",
    "The same sentence without stopwords conveys most of the meaning:\n",
    "\n",
    "\"cat ate mouse\"\n",
    "\n",
    "Below we are going to work through an example of removing stopwords from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the basic stopwords given by nltk\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set the language for stopwords\n",
    "# We have used a set() below, which is a data structure\n",
    "# that is useful for checking if some data is present somewhere else efficiently \n",
    "# (checking membership)\n",
    "stop_words_french = set(stopwords.words('french'))\n",
    "print(stop_words_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del stop_words_french\n",
    "# let's ensure we are using English stopwords for this project\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the default stopwords from `nltk` while useful, were created using domain knowledge and algorithms, rather than using a statistical method alone. Stopword collections are best custom made and specific for an individual application. The information on how it was curated is patchy at best. It was created for the [Snowball project](https://snowballstem.org/projects.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove stopwords from list of tokens\n",
    "def clean_stopwords(tokens):\n",
    "    # define stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # loop through each token and if the word isn't in the set \n",
    "    # of stopwords keep it\n",
    "    return [item for item in tokens if item not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre stopword removal\n",
    "patents['abstract_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['tokens_no_stops'] = patents['abstract_tokens'].apply(clean_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post stopword removal\n",
    "patents['tokens_no_stops'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were the stopwords that were removed\n",
    "found_stopwords = []\n",
    "\n",
    "# go through each unique token\n",
    "for token in set(patents['abstract_tokens'][0]):\n",
    "    if token in stop_words:\n",
    "        found_stopwords.append(token)\n",
    "        \n",
    "found_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Remove Specific Values\n",
    "\n",
    "We need to be able to remove specific parts of strings, such as numbers, apostrophies or other non-alphanumeric data. This can be done in a range of ways. \n",
    "\n",
    "We can either loop through tokens and remove / edit those with the values we don't want. Or we can use regular expressions (explained in the next chapter) to substitute our specific values in a string. Or a combination of both!\n",
    "\n",
    "Below we are going to show the looping method, as we haven't yet explored regex in detail. We will look at how removing tokens that are alphabetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that keeps only alphabetic text\n",
    "# creates new list for only tokes where alphanbetic == True\n",
    "\n",
    "def remove_num(ptokens):\n",
    "    return [token for token in ptokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some sample tokens, only one contains no digits\n",
    "alphanumeric = [\"Hello\", \"H3llo\", \"He11o\", \"Hell0\"]\n",
    "\n",
    "# remove_num detects the digits and removes those tokens\n",
    "remove_num(alphanumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing numbers this way will keep only words that have no digits or punctuation in them.\n",
    "print(\" \".join(remove_num(\"007 Not sure@ if this % was #fun! 558923 What do# you think** of it.? $500USD!\".split(\" \"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be careful in the situation above that we are only removing what we want to remove. How we alter / clean our text will impact the results of our analysis.\n",
    "\n",
    "If we wanted to remove tokens that were only digits, we could use the `token.isdigit()` method. There is rarely a one size fits all approach to cleaning, with trade offs in each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at a new text, row 21, which contains some digits\n",
    "patents[\"abstract\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply removal function\n",
    "patents['abstract_no_nums'] = patents['abstract_tokens'].apply(remove_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure you only use remove_num on tokenized text, otherwise it tokenizes every character.\n",
    "patents[\"abstract_tokens\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see there are no longer any tokens with digits in it\n",
    "# \"1x10\", \"-4\" have been removed\n",
    "patents[\"abstract_no_nums\"][21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Remove the words having length less than 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove words with two or fewer characters from a document. This could be useful in removing further words that are \"semantically empty\". This crosses over into stopword removal, but is more heavy handed.\n",
    "\n",
    "Some common words with only two characters \"to\", \"it\" carry very little meaning in some circumstances.\n",
    "\n",
    "We are goubg to create a function that keeps only strings of length $\\gt$ 2. This will be applied to example text, and then the patents data set. Depending on the distribution of word length, a processing step like this could have a big impact!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_tokens(ptokens):\n",
    "    return [token for token in ptokens if len(token) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a sentence with varying word length\n",
    "lyric = \"I'm blue, da ba dee da ba daa\"\n",
    "lyric_tokens = nltk.word_tokenize(lyric)\n",
    "lyric_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only \"longer\" words are kept\n",
    "remove_short_tokens(lyric_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the way our text has been normalised may have an impact on whether the token is kept. For example, if we have \"are\" -> \"is\", the new token would be removed by this rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre short word removal\n",
    "patents[\"abstract_tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the short word removal\n",
    "patents['abstract_no_small'] = patents['abstract_tokens'].apply(remove_short_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after removal\n",
    "patents[\"abstract_no_small\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting text above looks in a way \"cleaner\", it contains mostly words which have a semantic meaning, no punctuation and few non-sensical tokens. But is that useful to us? It depends on what we are trying to do. If we were analysing the frequency of punctuation.. probably not particularly useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Perform all the preprocessing steps wrapped into one function\n",
    "\n",
    "We can combine all of our preprocessing steps into one function for ease of use and reproducibility. It's important to note that the order we call each processing function matters here. If we remove short tokens early, we may find it harder to lemmatize the text. Without punctuation, we would be unable to tokenize sentences. Stopwords can be removed, but if the tokens have not been normalized somehow then they will not necessarily be picked up. We need to consider how we process text, and the effect of that processing carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_with_lemmatisation(raw_data):\n",
    "    \"\"\"Function to perform all preprocessing steps with lemmatisation\"\"\"\n",
    "    ptext = raw_data.lower()\n",
    "    ptext = remove_punct(ptext)\n",
    "    ptext = nltk.word_tokenize(ptext)\n",
    "    ptext = lemmatise(ptext)\n",
    "    ptext = remove_num(ptext)\n",
    "    ptext = clean_stopwords(ptext)\n",
    "    ptext = remove_short_tokens(ptext)\n",
    "\n",
    "    return ptext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_with_stemming(raw_data):\n",
    "    \"\"\"Function to perform all preprocessing steps with stemming\"\"\"\n",
    "    ptext = raw_data.lower()\n",
    "    ptext = remove_punct(ptext)\n",
    "    ptext = nltk.word_tokenize(ptext)\n",
    "    ptext = stemming(ptext)\n",
    "    ptext = remove_num(ptext)\n",
    "    ptext = clean_stopwords(ptext)\n",
    "    ptext = remove_short_tokens(ptext)\n",
    "        \n",
    "    return ptext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform all processing at once\n",
    "patents['processed_with_lem'] = patents['abstract'].apply(preprocessing_with_lemmatisation)\n",
    "\n",
    "patents[\"processed_with_lem\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['processed_with_stem'] = patents['abstract'].apply(preprocessing_with_stemming)\n",
    "\n",
    "patents[\"processed_with_stem\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Compare the results of the different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Often in practice we can combine many of our preprocessing, cleaning and normalisation steps into a single pipeline that will reproducibly apply the same steps to different data.\n",
    "\n",
    "We can:\n",
    "\n",
    "* Combine steps into functions as shown above\n",
    "* Use packages such as `sklearn` to build analysers and processors such as in [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) (more covered in Intermediate NLP).\n",
    "* Use a spaCy pipeline (module 5)\n",
    "* Use a Stanza pipeline (shown below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Using Stanza (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The Stanza parts of this course are optional based on software requirements. The Stanza material is useful to be aware of but not crucial to your understanding of basic NLP.\n",
    "\n",
    "Stanza is a Python natural language analysis package. It contains tools which can be used in a pipeline in order to:\n",
    "\n",
    "* convert a string containing text into lists of sentences and words.\n",
    "* generate base forms of words.\n",
    "* generate parts of speech and morphological features.\n",
    "* give a syntactic structure dependency parse.\n",
    "* recognize named entities.\n",
    "\n",
    "(*adapted from Stanford University, 2020*).\n",
    "<br>\n",
    "\n",
    "<img src=\"../pics/stanzapipeline.png\" alt=\"Flow chart of a typical Stanze pipeline\" width=550>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Some example on how to use stanza for language processing shown below - further examples highlighted in the language syntax and structure module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "\n",
    "To start annotating text with Stanza, you would typically start by building a Pipeline that contains Processors, each fulfilling a specific NLP task you desire (e.g., tokenization, part-of-speech tagging, syntactic parsing, etc). The pipeline takes in raw text or a Document object that contains partial annotations, runs the specified processors in succession, and returns an annotated Document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processors\n",
    "\n",
    "Processors are units of the neural pipeline that perform specific NLP functions and create different annotations for a Document. The neural pipeline supports the following processors:\n",
    "\n",
    "tokenize, mwt(expands multi word expressions), pos(part of speech), lemma, depparse(dependency parsing),\n",
    "ner(named entity recognition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the English model download below requires ~0.5 Gb of memory\n",
    "# For some networked devices the code below will not run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stanza.download('en') # download English model\n",
    "#nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.\") # run annotation over a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Tokenisation and sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "doc = nlp('This is a test sentence for stanza. This is another sentence.')\n",
    "\n",
    "for index, sentence in enumerate(doc.sentences, start=1):\n",
    "    print(f'====== Sentence {index} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "doc = nlp('Barack Obama was born in Hawaii.')\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Challenges\n",
    "\n",
    "As mentioned previously, with each processing technique there are challenges associated or areas where the technique performs poorly. It is important to be aware of what these could be for each step in your processing pipeline.\n",
    "<br>\n",
    "\n",
    "#### 2.12 Punctuation\n",
    "Words like Ph.D that have a ., but the sentence does not finish would require an exception function. Additionally words like \"don't\", \"won't\" also need to be handled with caution.\n",
    "\n",
    "We can either create rules for each of these situations (many built in to modern preprocessing methods). Learn the rules (with machine learning methods) or create lexicons (files containing text data) which have mappings for specific cases.\n",
    "\n",
    "#### 2.13 Consistency\n",
    "Using different methods for lemmatization may give different results- staying consistent throughout your work will ease your processing and will not mess with your results. \n",
    "\n",
    "Good programming documentation can help with this, as well as exploratory analysis and evaluation of our chosen methods. There is rarely a one size fits all choice for text normalisation. \n",
    "\n",
    "#### 2.14 Stemming\n",
    "Usually stemming is not preferred. If you do want to use stemming to help you find more words that are closely related, then it would be better if you keep the stemmised and the non-stemmised version of the word. This will help you present the results as the end.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "<br>\n",
    "\n",
    "<ol>\n",
    "  <li>Import the Hep Dataset and perform the following preprocessing steps to the \"Text\" column.</li>\n",
    "    \n",
    "        Lowercasing\n",
    "        Remove Punctuation\n",
    "        Tokenize\n",
    "        Lemmatization\n",
    "        Stemming\n",
    "        Stopword Removal\n",
    "        Remove Numbers\n",
    "        Remove the words having length less than 2\n",
    "        Tokenise, sentence splitting and lemmatisation using Stanza\n",
    " \n",
    "</ol>\n",
    "\n",
    "Guidelines: \n",
    "\n",
    "* Change the \"Text\" columnn from list to a string before undertaking pre-processing.  <br>\n",
    "* Perform the preprocessing steps in the same way as done to the patent dataset abstract column. <br>\n",
    "* Once punctuation removal, tokenisation, lemmatisation, stemming undertaken put the results in new columns in the df. <br> \n",
    "* Apply lemmatisation and stemming on text that has been tokenised <br> \n",
    "* Make a copy of the df once loaded in using copy() <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep = pd.read_pickle(\"../data/Hep_Dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "\n",
    "https://mc.ai/text-preprocessing-for-nlp-and-machine-learning-tasks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad_NLP_2021",
   "language": "python",
   "name": "grad_nlp_2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
